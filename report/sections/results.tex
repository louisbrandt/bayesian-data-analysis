\section{Results}

An arsenal of models fitted to various subsets of the data, each with their own
explicit assumptions, sets a foundation for (+necessitates) a rigorous evaluation
and discussion of results and their implications. This section highlights notable
findings and applies the lens of bayesian model comparison to bring order and 
to interpret the performance of the models. 

Metrics that should be familiar to the reader are used to quantify the predictive
power of the models, such as the mean absolute error (MAE), while bayesian model 
comparison tools are employed to answer the research questions posed in the 
introduction, LOO validation \& WAIC.

\subsection{Model Comparison}

The primary research question of this study is to determine the best-performing
model for the case study, predicting the restaurant chain's revenue. In the
Bayesian framework, both the choice of data to fit a model and the model itself
are intertwined. The observations are integral in shaping the posterior
distribution through the likelihood function, so the model and the inferred
posterior of the model parameters do not exist separately from the data.
With many plausible models fit on the same data, the question of which model is
best can be answered by applying Bayesian model comparison techniques, as
described in \cite{statrethinking} Chapter 7.5. The seamless application of
this comparison and selection is an attractive facet of working within the
Bayesian system.
However, the volume of data itself is also a core subject of this paper and
case study. To compare models fit on different subsets of the data, we can
borrow ideas from frequentist approaches like mean squared error (MSE) and mean
absolute error (MAE).
In the Bayesian context, we can assess the out-of-sample predictive accuracy of
the models, even though Bayesian models do not have a separate 'out-of-sample'
by definition. Techniques such as cross-validation or information criteria
(e.g., WAIC and LOO-CV) can approximate the out-of-sample predictive accuracy
without explicitly holding out data during the modeling process.

To identify the best model, we can examine its predictive performance on unseen
data, mimicking real-world scenarios. To do this, we can address the following
questions: How close does the model predict the actual value? How well does the
model explain the true data? And how well does the model predict future data?

By evaluating the models using both Bayesian model comparison techniques and
frequentist measures of performance on different subsets of data, we can
effectively determine which model best predicts the restaurant chain's revenue
and addresses the primary research question of this study.
